{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"figs/LogoUFSCar.jpg\" alt=\"Logo UFScar\" width=\"110\" align=\"left\"/>  <br/> <center>Universidade Federal de São Carlos (UFSCar)<br/><font size=\"4\"> Departamento de Computação, campus Sorocaba</center></font>\n",
    "</p>\n",
    "\n",
    "<font size=\"4\"><center><b>Disciplina: Aprendizado de Máquina</b></center></font>\n",
    "  \n",
    "<font size=\"3\"><center>Prof. Dr. Tiago A. Almeida</center></font>\n",
    "\n",
    "## <center>Projeto Final</center>\n",
    "\n",
    "**Aluno**: Vitor Silveira\n",
    "\n",
    "**RA**: 802318\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise exploratória\n",
    "\n",
    "Nesta seção, deve ser feita a leitura da base de dados e todas as análises necessárias para interpretar e analisar os dados, tais como:\n",
    "* Significado de cada atributo\n",
    "* Medidas descritivas\n",
    "* Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "diretorio = 'data/raw_data'\n",
    "\n",
    "arquivos_csv = [arquivo for arquivo in os.listdir(diretorio) if arquivo.startswith('new') and arquivo.endswith('.csv')]\n",
    "\n",
    "dados_concatenados = pd.DataFrame()\n",
    "\n",
    "for arquivo in arquivos_csv:\n",
    "   print(arquivo)\n",
    "   caminho_arquivo = os.path.join(diretorio, arquivo)\n",
    "   dados = pd.read_csv(caminho_arquivo)\n",
    "   dados_concatenados = pd.concat([dados_concatenados, dados], ignore_index=True)\n",
    "\n",
    "alldata = dados_concatenados\n",
    "dados_concatenados.to_csv(f'data/pre_processed_data/all_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\"data/raw_data/train.csv\")                    # É o Y (com os labels)\n",
    "\n",
    "test_csv = pd.read_csv(\"data/raw_data/test.csv\")                      # Teste\n",
    "\n",
    "sample_sub = pd.read_csv(\"data/raw_data/sample_submission.csv\")       # Submissão\n",
    "\n",
    "alldata = pd.read_csv(\"data/pre_processed_data/all_news.csv\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_csv.merge(alldata, on = \"id\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinned_train = train_csv.merge(alldata, on = \"id\")\n",
    "joinned_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Pré-processamento\n",
    "\n",
    "Nesta seção, as funções da etapa de pré-processamento dos dados devem ser implementadas e aplicadas (se necessário)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinned_train.dropna(inplace = True)\n",
    "joinned_train.drop(columns = [ \"id\", \"date\" ], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(columns = [ \"id\", \"date\" ], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinned_train[\"title_content\"] = joinned_train[\"title\"] + \" \" + joinned_train[\"content\"]\n",
    "test[\"title_content\"] = test[\"title\"] + \" \" + test[\"content\"]\n",
    "\n",
    "joinned_train.drop(columns = [ \"title\", \"content\" ], inplace = True)\n",
    "test.drop(columns = [ \"title\", \"content\" ], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "rm_punctuation = string.punctuation\n",
    "\n",
    "stopw = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stopw])\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', rm_punctuation))\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    text = to_lowercase(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinned_train[\"title_content\"] = joinned_train[\"title_content\"].apply(lambda text: preprocess(text))\n",
    "test[\"title_content\"] = test[\"title_content\"].apply(lambda text: preprocess(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Experimento\n",
    "\n",
    "Nesta seção, o experimento deve ser conduzido, utilizando os protocolos experimentais padrões e testando diferentes modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = joinned_train[\"title_content\"].astype(str)\n",
    "y_train = joinned_train[\"label\"]\n",
    "X_test = test[\"title_content\"].apply(lambda x: np.str_(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorization = TfidfVectorizer(max_features = 8000)\n",
    "x_train = vectorization.fit_transform(X)\n",
    "X_test_vec = vectorization.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(objective='binary:logistic')\n",
    "xgb.fit(x_train, y_train)\n",
    "predXGB = xgb.predict_proba(X_test_vec)\n",
    "XGBscore = xgb.score(x_train, y_train)\n",
    "print(XGBscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000).fit(x_train, y_train)\n",
    "print(\"passo 1\")\n",
    "predRL = clf.predict_proba(X_test_vec)\n",
    "print(\"passo 2\")\n",
    "LRscore = clf.score(x_train, y_train)\n",
    "print(LRscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(\"data/raw_data/sample_submission.csv\")   \n",
    "sample_sub[\"label\"] = predXGB[:, 1] \n",
    "sample_sub.set_index(\"id\").to_csv(\"default-xgb-alldata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Análise dos Resultados\n",
    "\n",
    "Nesta seção, os resultados devem ser exibidos através de tabelas e gráficos, comparados e profundamente analisados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
